vector[V] normalized_probs = expected_counts;
X[d] ~ multinomial(normalized_probs);
}
}
"
library(rstan)
# Compile the Stan model
stan_model <- stan_model(model_code = stan_model_code)
# Compile the Stan model
#stan_model <- stan_model(model_code = stan_model_code)
# Fit the model
# Fit the model using VBEM
fit <- vb(stan_model, data = data_list, iter = 10000, output_samples = 1000)
# Define the Stan model as a string
stan_model_code <- "
data {
int<lower=1> N1; // Number of reviewers
int<lower=1> N2; // Number of papers
int<lower=1> V; // Vocabulary size
int<lower=1> K; // Number of topics
int<lower=1> K1; // Number of reviewer types
int<lower=1> K2; // Number of paper categories
int<lower=1> D; // Total number of reviews (documents)
int<lower=0> X[D, V]; // Document-term matrix (word counts)
int<lower=1> reviewer_ids[D]; // Reviewer indices for each document
int<lower=1> paper_ids[D]; // Paper indices for each document
}
parameters {
simplex[V] phi[K]; // Word distributions for each topic
simplex[K1] reviewer_type[N1]; // Reviewer type distribution
simplex[K2] paper_category[N2]; // Paper category distribution
simplex[K] topic_assignment[N1 * N2]; // Topic assignment matrices for each level of the third dimension
simplex[K] core[K1 * K2]; // Core
}
model {
// Priors
for (k in 1:K)
phi[k] ~ dirichlet(rep_vector(0.01, V));
for (n in 1:N1)
reviewer_type[n] ~ dirichlet(rep_vector(0.01, K1));
for (n in 1:N2)
paper_category[n] ~ dirichlet(rep_vector(0.01, K2));
for (n1 in 1:N1){
for (n2 in 1:N2){
topic_assignment[(n1-1) * N2 + n2] ~ dirichlet(rep_vector(0.01, K));
}
}
for (k1 in 1:K1){
for (k2 in 1:K2){
core[(k1-1) * K2 + k2] ~ dirichlet(rep_vector(0.01, K));
}
}
// Likelihood using document-term matrix X
for (d in 1:D) {
vector[K] topic_probs;  // Initialize topic_probs as a vector of size K3
for (k3 in 1:K) {
real sum_terms = 0.0;  // Initialize the summation term for each k3
for (k1 in 1:K1) {
for (k2 in 1:K2) {
sum_terms += core[(k1-1) * K2 + k2, k3] * reviewer_type[reviewer_ids[d], k1] * paper_category[paper_ids[d], k2];
}
}
topic_probs[k3] = sum_terms * topic_assignment[d, k3];
}
// Compute the likelihood using the calculated topic_probs
vector[V] expected_counts;
for (v in 1:V) {
expected_counts[v] = dot_product(to_vector(phi[, v]), topic_probs);
}
// Normalize expected counts to get probabilities
vector[V] normalized_probs = expected_counts / sum(expected_counts);
//vector[V] normalized_probs = expected_counts;
X[d] ~ multinomial(normalized_probs);
}
}
"
library(rstan)
# Compile the Stan model
stan_model <- stan_model(model_code = stan_model_code)
fit <- vb(stan_model, data = data_list, iter = 10000, output_samples = 1000)
#### Apply SPOC
A1_hat <- fit_SPOC(W3_modified/ Q2, K1)
## other methods
library(nnTensor)
library(tidyverse)
library(topicmodels)
library(tidytext)
source("~/Documents/tensor-topic-modeling/PLSI.r", echo=TRUE)
A1 <- data$A1
A2 <- data$A2
A3 <- data$A3
core <- data$G
Y <- data$Y
Q1 <- dim(A1)[1]
Q2 <- dim(A2)[1]
#R_old = R
R <- dim(A3)[1]
D <- data$Y/ M
D3  <- matricization(Y, 3)
D1=matrization_tensor(D,1)
D2=matrization_tensor(D,2)
lda3 <- LDA(t(D3), k = K3, control = list(seed = 1234), method = 'VEM')
source("~/Documents/tensor-topic-modeling/tensor_operations.r", echo=TRUE)
D3  <- matricization(Y, 3)
D1=matrization_tensor(D,1)
D2=matrization_tensor(D,2)
lda3 <- LDA(t(D3), k = K3, control = list(seed = 1234), method = 'VEM')
ap_topics3 <- tidy(lda3, matrix = "beta")
W3 = tidy(lda3, matrix = "gamma")
W3 <- pivot_wider(W3, id_cols = "document",
names_from = "topic",
values_from = "gamma")
##### sum across k_3
#W3_modified <- W3 %>%
#  group_by(document) %>%
#  summarise(gamma = sum(gamma))
W3_modified = W3
W3_modified["dim1"] = unlist(lapply(1:Q1, function(x){rep(x, Q2)}))
W3_modified["dim2"] = unlist(lapply(1:Q1, function(x){1:Q2}))
W3_modified = W3_modified %>%
group_by(dim1) %>%
summarise(`1` = sum(`1`),
`2` = sum(`2`),
`3` = sum(`3`),
`4` = sum(`4`))
#### Apply SPOC
A1_hat <- fit_SPOC(W3_modified/ Q2, K1)
W3_modified_bis = W3
W3_modified_bis["dim1"] = unlist(lapply(1:Q1, function(x){rep(x, Q2)}))
W3_modified_bis["dim2"] = unlist(lapply(1:Q1, function(x){1:Q2}))
W3_modified_bis = W3_modified_bis %>%
group_by(dim2) %>%
summarise(`1` = sum(`1`),
`2` = sum(`2`),
`3` = sum(`3`),
`4` = sum(`4`))
#### Apply SPOC
A2_hat <- fit_SPOC(W3_modified_bis/ Q1, K2)
params <- extract(fit)
# Access specific parameters
phi_samples <- params$phi       # Word distributions for each topic
reviewer_type_samples <- params$reviewer_type  # Reviewer type distribution
paper_category_samples <- params$paper_category  # Paper category distribution
topic_assignment_samples <- params$topic_assignment  # Topic assignment matrices
core_samples <- params$core     # Core tensor
# Example: Plotting the reviewer type distributions (reviewer_type)
reviewer_type_mean <- apply(reviewer_type_samples, c(2, 3), mean)  # Mean across iterations
# Convert to long format
reviewer_type_df <- melt(reviewer_type_mean)
reviewer_type_mean
pivot_longer(reviewer_type_mean)
help(melt)
colnames(reviewer_type_mean) = c("Type 1", "Type 2")
pivot_longer(reviewer_type_mean)
N1
reviewer_type_mean["sample"] = sapply(1:N1, function(x){paste0("Reviewer ", x)})
dim(reviewer_type_mean)
# Example: Plotting the reviewer type distributions (reviewer_type)
reviewer_type_mean <- apply(reviewer_type_samples, c(2, 3), mean)  # Mean across iterations
reviewer_type_mean = data.frame(reviewer_type_mean)
nrow(reviewer_type_mean)
reviewer_type_mean["sample"] = sapply(1:N1, function(x){paste0("Reviewer ", x)})
reviewer_type_mean
hlep(pivot_longer)
help("pivot_longer")
# Convert to long format
reviewer_type_df <- pivot_longer(reviewer_type_mean, cols = -c("sample"))
reviewer_type_df
colnames(reviewer_type_df) <- c("Reviewer", "Reviewer_Type", "Probability")
# Plot the reviewer type distributions
ggplot(reviewer_type_df, aes(x = Reviewer_Type, y = Probability, fill = factor(Reviewer))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Reviewer Type Distributions", x = "Reviewer Type", y = "Probability") +
theme_minimal()
# Plot the reviewer type distributions
ggplot(reviewer_type_df, aes(x = Reviewer, y = Probability, fill = factor(Reviewer_Type))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Reviewer Type Distributions", x = "Reviewer Type", y = "Probability") +
theme_minimal()
# Plot the reviewer type distributions
ggplot(reviewer_type_df, aes(x = Reviewer, y = Probability, fill = factor(Reviewer_Type))) +
geom_bar(stat = "identity", position = "stacked") +
labs(title = "Reviewer Type Distributions", x = "Reviewer Type", y = "Probability") +
theme_minimal()
# Plot the reviewer type distributions
ggplot(reviewer_type_df, aes(x = Reviewer, y = Probability, fill = factor(Reviewer_Type))) +
geom_bar(stat = "identity", position = "stack") +
labs(title = "Reviewer Type Distributions", x = "Reviewer Type", y = "Probability") +
theme_minimal()
# Example: Plotting the paper category distributions (paper_category)
paper_category_mean <- apply(paper_category_samples, c(2, 3), mean)  # Mean across iterations
colnames(paper_category_mean) = sapply(1:K2, function(x){paste0("Category ", x)})
colnames(paper_category_mean)
paper_category_mean["paper"] = sapply(1:N2, function(x){paste0("Paper ", x)})
colnames(paper_category_mean) = sapply(1:K2, function(x){paste0("Category ", x)})
# Convert to long format
paper_category_mean = data.frame(paper_category_mean)
colnames(paper_category_mean) = sapply(1:K2, function(x){paste0("Category ", x)})
# Convert to long format
paper_category_mean = data.frame(paper_category_mean)
# Example: Plotting the paper category distributions (paper_category)
paper_category_mean <- apply(paper_category_samples, c(2, 3), mean)  # Mean across iterations
# Convert to long format
paper_category_mean = data.frame(paper_category_mean)
colnames(paper_category_mean) = sapply(1:K2, function(x){paste0("Category ", x)})
paper_category_mean["paper"] = sapply(1:N2, function(x){paste0("Paper ", x)})
# Convert to long format
paper_category_df <- pivot_longer(paper_category_mean, cols = -c("paper"))
colnames(paper_category_df) <- c("Paper", "Paper_Category", "Probability")
# Plot the paper category distributions
ggplot(paper_category_df, aes(x = Paper_Category, y = Probability, fill = factor(Paper))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Paper Category Distributions", x = "Paper Category", y = "Probability") +
theme_minimal()
# Plot the paper category distributions
ggplot(paper_category_df, aes(x = Paper, y = Probability, fill = factor(Paper_Category))) +
geom_bar(stat = "identity", position = "stack") +
labs(title = "Paper Category Distributions", x = "Paper Category", y = "Probability") +
theme_minimal()
K3
# Example: Plotting the word distributions for each topic (phi)
phi_mean <- apply(phi_samples, c(2, 3), mean)  # Take the mean across iterations
phi_mean = data.frame(phi_mean)
colnames(phi_mean) = sapply(1:K3, function(x){paste0("Topc ", x)})
R
colnames(phi_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
phi_mean["word"] = sapply(1:R, function(x){paste0("Word ", x)})
R
# Example: Plotting the word distributions for each topic (phi)
phi_mean <- apply(phi_samples, c(2, 3), mean)  # Take the mean across iterations
phi_mean = data.frame(phi_mean)
colnames(phi_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
phi_mean["word"] = sapply(1:R, function(x){paste0("Word ", x)})
dim(phi_mean)
# Example: Plotting the word distributions for each topic (phi)
phi_mean <- apply(phi_samples, c(2, 3), mean)  # Take the mean across iterations
phi_mean = t(phi_mean)
phi_mean = data.frame(phi_mean)
colnames(phi_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
phi_mean["word"] = sapply(1:R, function(x){paste0("Word ", x)})
# Convert to long format
phi_mean_df <- pivot_longer(phi_mean, cols = -c("Word"))
# Convert to long format
phi_mean_df <- pivot_longer(phi_mean, cols = -c("word"))
colnames(phi_mean_df) <- c("word", "Topic", "Probability")
# Convert to long format for ggplot2
phi_df <- melt(phi_mean)
# Plot the word distributions for each topic
ggplot(phi_df, aes(x = Word, y = Probability, fill = factor(Topic))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Word Distributions for Each Topic (Phi)", x = "Word", y = "Probability") +
theme_minimal()
# Plot the word distributions for each topic
ggplot(phi_mean_df, aes(x = Word, y = Probability, fill = factor(Topic))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Word Distributions for Each Topic (Phi)", x = "Word", y = "Probability") +
theme_minimal()
# Plot the word distributions for each topic
ggplot(phi_mean_df, aes(x = Word, y = Probability, fill = factor(Topic))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Word Distributions for Each Topic (Phi)", x = "word", y = "Probability") +
theme_minimal()
# Plot the word distributions for each topic
ggplot(phi_mean_df, aes(x = word, y = Probability, fill = factor(Topic))) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Word Distributions for Each Topic (Phi)", x = "Word", y = "Probability") +
theme_minimal()
# Plot the word distributions for each topic
ggplot(phi_mean_df, aes(x = word, y = Probability, fill = factor(Topic))) +
geom_bar(stat = "identity", position = "stack") +
labs(title = "Word Distributions for Each Topic (Phi)", x = "Word", y = "Probability") +
theme_minimal()
# Define the Stan model as a string
stan_model_code <- "
data {
int<lower=1> N1; // Number of reviewers
int<lower=1> N2; // Number of papers
int<lower=1> V; // Vocabulary size
int<lower=1> K; // Number of topics
int<lower=1> K1; // Number of reviewer types
int<lower=1> K2; // Number of paper categories
int<lower=1> D; // Total number of reviews (documents)
int<lower=0> X[D, V]; // Document-term matrix (word counts)
int<lower=1> reviewer_ids[D]; // Reviewer indices for each document
int<lower=1> paper_ids[D]; // Paper indices for each document
}
parameters {
simplex[V] phi[K]; // Word distributions for each topic
simplex[K1] reviewer_type[N1]; // Reviewer type distribution
simplex[K2] paper_category[N2]; // Paper category distribution
//simplex[K] topic_assignment[N1 * N2]; // Topic assignment matrices for each level of the third dimension
simplex[K] core[K1 * K2]; // Core
}
model {
// Priors
for (k in 1:K)
phi[k] ~ dirichlet(rep_vector(0.01, V));
for (n in 1:N1)
reviewer_type[n] ~ dirichlet(rep_vector(0.01, K1));
for (n in 1:N2)
paper_category[n] ~ dirichlet(rep_vector(0.01, K2));
for (k1 in 1:K1){
for (k2 in 1:K2){
core[(k1-1) * K2 + k2] ~ dirichlet(rep_vector(0.01, K));
}
}
// Likelihood using document-term matrix X
for (d in 1:D) {
vector[K] topic_probs;  // Initialize topic_probs as a vector of size K3
for (k3 in 1:K) {
real sum_terms = 0.0;  // Initialize the summation term for each k3
for (k1 in 1:K1) {
for (k2 in 1:K2) {
sum_terms += core[(k1-1) * K2 + k2, k3] * reviewer_type[reviewer_ids[d], k1] * paper_category[paper_ids[d], k2];
}
}
topic_probs[k3] = sum_terms;
}
// Compute the likelihood using the calculated topic_probs
vector[V] expected_counts;
for (v in 1:V) {
expected_counts[v] = dot_product(to_vector(phi[, v]), topic_probs);
}
// Normalize expected counts to get probabilities
//vector[V] normalized_probs = expected_counts / sum(expected_counts);
vector[V] normalized_probs = expected_counts;
X[d] ~ multinomial(normalized_probs);
}
}
"
library(rstan)
# Compile the Stan model
stan_model <- stan_model(model_code = stan_model_code)
fit <- vb(stan_model, data = data_list, iter = 10000, output_samples = 1000)
params <- extract(fit)
# Access specific parameters
phi_samples <- params$phi       # Word distributions for each topic
params$phi
fit <- sampling(stan_model, data = data_list, iter = 2000, chains = 4)
params <- extract(fit)
# Access specific parameters
topics <- params$phi       # Word distributions for each topic
reviewer_type_samples <- params$reviewer_type  # Reviewer type distribution
paper_category_samples <- params$paper_category  # Paper category distribution
core_samples <- params$core     # Core tensor
# Extracting the reviewer type distributions (reviewer_type)
reviewer_type_mean <- apply(reviewer_type_samples, c(2, 3), mean)  # Mean across iterations
reviewer_type_mean = data.frame(reviewer_type_mean)
colnames(reviewer_type_mean) = c("Type 1", "Type 2")
reviewer_type_mean["sample"] = sapply(1:N1, function(x){paste0("Reviewer ", x)})
# Convert to long format
reviewer_type_df <- pivot_longer(reviewer_type_mean, cols = -c("sample"))
colnames(reviewer_type_df) <- c("Reviewer", "Reviewer_Type", "Probability")
#  Extracting the paper category distributions (paper_category)
paper_category_mean <- apply(paper_category_samples, c(2, 3), mean)  # Mean across iterations
# Convert to long format
paper_category_mean = data.frame(paper_category_mean)
colnames(paper_category_mean) = sapply(1:K2, function(x){paste0("Category ", x)})
paper_category_mean["paper"] = sapply(1:N2, function(x){paste0("Paper ", x)})
# Convert to long format
paper_category_df <- pivot_longer(paper_category_mean, cols = -c("paper"))
colnames(paper_category_df) <- c("Paper", "Paper_Category", "Probability")
# Extracting the word distributions for each topic (phi)
phi_mean <- apply(phi_samples, c(2, 3), mean)  # Take the mean across iterations
phi_mean = t(phi_mean)
phi_mean = data.frame(phi_mean)
colnames(phi_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
phi_mean["word"] = sapply(1:R, function(x){paste0("Word ", x)})
# Convert to long format
phi_mean_df <- pivot_longer(phi_mean, cols = -c("word"))
colnames(phi_mean_df) <- c("word", "Topic", "Probability")
# Extracting the core
core_mean <- apply(core_samples, c(2, 3), mean)  # Take the mean across iterations
core_mean = t(core_mean)
core_mean = data.frame(core_mean)
colnames(core_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
dim(core_mean)
K3
K1
K2
# Extracting the core
core_mean <- apply(core_samples, c(2, 3), mean)  # Take the mean across iterations
dim(core_mean)
core_mean = data.frame(core_mean)
colnames(core_mean) = sapply(1:K3, function(x){paste0("Topic ", x)})
core_mean
core_mean["cluster1"] =  rep(1:K1, each = K2)
core_mean
core_mean["cluster2"] =  rep(1:K2, times = K1)
core_mean
# Convert to long format
core_mean_df <- pivot_longer(core_mean, cols = -c("cluster1", "cluster2"))
core_mean_df
colnames(core_mean_df) <- c("cluster1", "cluster2", "Topic", "Probability")
core_mean_df
lda3 <- LDA(t(D3), k = K3,
control = list(seed = 1234), method = 'VEM')
ap_topics3 <- tidy(lda3, matrix = "beta")
W3 = tidy(lda3, matrix = "gamma")
W3 <- pivot_wider(W3, id_cols = "document",
names_from = "topic",
values_from = "gamma")
W3_modified = W3
W3_modified["dim1"] = unlist(lapply(1:Q1, function(x){rep(x, Q2)}))
W3_modified["dim2"] = unlist(lapply(1:Q1, function(x){1:Q2}))
W3_modified
help("pivot_wider")
help("pivot_longer")
W3_modified %>% select(-c("document"))
pivot_longer(W3_modified %>% select(-c("document")), cols = -c( "dim1"))
W3_modified
pivot_wider()
help("pivot_wider")
df_long <- W3_modified %>%
pivot_longer(cols = starts_with("1"):starts_with("4"),
names_to = "topic",
values_to = "value")
print(df_long)
#### Transform W3 into a tensor
W3_modified = W3_modified %>%
pivot_longer(cols = starts_with("1"):starts_with("4"),
names_to = "topic",
values_to = "value")
W3_modified
W3_modified %>%
unite("dim2_topic", dim2, topic, sep = "_topic_") %>%
pivot_wider(names_from = dim2_topic, values_from = value)
W3_modified %>%
select(-c("document"))%>%
unite("dim2_topic", dim2, topic, sep = "_topic_") %>%
pivot_wider(names_from = dim2_topic, values_from = value)
W3_modified <- W3_modified %>%
select(-c("document"))%>%
unite("dim2_topic", dim2, topic, sep = "_topic_") %>%
pivot_wider(names_from = dim2_topic, values_from = value)
W3_modified
W3_modified %>% select(-c("dim1"))/ Q2
fit_SPOC(W3_modified %>% select(-c("dim1"))/ Q2, K1)
W3_modified = W3
W3_modified["dim1"] = unlist(lapply(1:Q1, function(x){rep(x, Q2)}))
W3_modified["dim2"] = unlist(lapply(1:Q1, function(x){1:Q2}))
#### Transform W3 into a tensor
W3_modified = W3_modified %>%
pivot_longer(cols = starts_with("1"):starts_with("4"),
names_to = "topic",
values_to = "value")
print(df_long)
W3_modified_A1 <- W3_modified %>%
select(-c("document"))%>%
unite("dim2_topic", dim2, topic, sep = "_topic_") %>%
pivot_wider(names_from = dim2_topic, values_from = value)
W3_modified_A2 <- W3_modified %>%
select(-c("document"))%>%
unite("dim1_topic", dim1, topic, sep = "_topic_") %>%
pivot_wider(names_from = dim1_topic, values_from = value)
W3_modified_A2
#### Apply SPOC
A2_hat <- fit_SPOC(W3_modified_A2 %>% select(-c("dim2"))/ Q1, K2)
A2_hat
##### Estimate the core through regression
# Compute the Kronecker product
A1A2 <- kronecker(A1_hat, A2_hat)
A1_hat
##### Estimate the core through regression
# Compute the Kronecker product
A1A2 <- kronecker(A1_hat$What, A2_hat$What)
A1A2
install.packages("CVXR")
library(CVXR)
# Define the beta variable as a matrix with K1 * K2 rows and K3 columns
beta <- Variable(K1 * K2, K3)
dim(W3)
K
W3
W3 %>% select(-c("document"))
dim(A1A2)
K1 * K2
# Objective: minimize the sum of squared residuals
objective <- Minimize(sum_squares(W3 %>% select(-c("document")) - A1A2 %*% beta))
# Objective: minimize the sum of squared residuals
objective <- Minimize(sum_squares(as.matrix(W3 %>% select(-c("document"))) - A1A2 %*% beta))
# Constraints: non-negativity and row sums equal to 1
constraints <- list(beta >= 0, rowSums(beta) == 1)
constraints <- list(beta >= 0, sum_entries(beta, axis = 2) == 1)
# Define the problem
problem <- Problem(objective, constraints)
# Solve the problem
result <- solve(problem)
# Get the optimized beta
beta_optimized <- matrix(result$getValue(beta), nrow = K1 * K2, ncol = K3)
beta_optimized
beta_optimized[which(beta_optimized < 1e-10)] = 0
beta_optimized
ap_topics3
ap_topics3
pivot_wider(ap_topics3, names_from = "topic", values_from = "beta")
ap_topics3 <- tidy(lda3, matrix = "beta")
ap_topics3
dim(D3)[1]
R
ap_topics3["word"] = sapply(1:R, function(x){rep(x, K3)})
x=1;rep(x, K3)
sapply(1:R, function(x){rep(x, K3)})
lapply(1:R, function(x){rep(x, K3)})
unlist(lapply(1:R, function(x){rep(x, K3)}) )
ap_topics3["word"] = unlist(lapply(1:R, function(x){rep(x, K3)}))
ap_topics3
ap_topics3 = pivot_wider(ap_topics3, id_cols = "word", names_from = "topic",
values_from = "beta")
ap_topics3
install.packages("tensr")
q()
