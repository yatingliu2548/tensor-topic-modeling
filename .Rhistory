view_matrix(res_grad$projection[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
lambda = 7 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 5, nv=5)
Sigma_hat = test$v %*% diag(test$d[1:5]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
lambda = 7 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 2, nv=2)
Sigma_hat = test$v %*% diag(test$d[1:2]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
lambda = 10 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 2, nv=2)
Sigma_hat = test$v %*% diag(test$d[1:2]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
test$d
lambda = 20 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 2, nv=2)
Sigma_hat = test$v %*% diag(test$d[1:2]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
# Verify the result
view_matrix(Smat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
# Verify the result
view_matrix(Smat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
view_matrix(z[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
view_evec(
test$v,
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
lambda = 10 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 2, nv=2)
Sigma_hat = test$v %*% diag(test$d[1:2]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
view_evec(
test$v[, 1:100],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
test$v[, 1:100]
dim(test$v)
view_evec(
test$v[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
test$u[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
lambda = 30 * sqrt(log(p) / n)
lambda = 5 * sqrt(log(p) / n)
hatC = z
hatC[which(abs(z) <lambda) ]=0
hatC[which(z < (-lambda)) ]= z[which(z < (-lambda)) ] +lambda
hatC[which(z > (lambda)) ]= z[which(z > (lambda)) ] - lambda
test = svd(hatC, nu = 2, nv=2)
Sigma_hat = test$v %*% diag(test$d[1:2]^2) %*% t(test$v)
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
view_evec(
test$v[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
test$u[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
# Verify the result
view_matrix(Smat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
library(ssvd)
install.packages("~/Downloads/ssvd_1.0 (2).tar.gz", repos = NULL, type = "source")
library(ssvd)
ssvd.iter.thresh(z)
source("~/my_ssvd.R", echo=TRUE)
ssvd(z)
test = ssvd(z, method = "theory")
test
test$u
test = ssvd(z, method = "theory",r = 4)
test$d
dim(test$v )
view_matrix(test$v %*% diag(test$d) %*% t(test$v ), legend_title = "GradFPS\nProjection\nMatrix")
Sigma_hat = test$v %*% diag(test$d) %*% t(test$v )
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
test = ssvd(z, method = "method",r = 4)
Sigma_hat = test$v %*% diag(test$d) %*% t(test$v )
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
test = ssvd(z, method = "theory",r = 4)
Sigma_hat = test$v %*% diag(test$d) %*% t(test$v )
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
test = ssvd(z, method = "theory",r = 2)
Sigma_hat = test$v %*% diag(test$d) %*% t(test$v )
view_matrix(Sigma_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
dim(test$u)
SigmaU_hat = test$u %*% diag(test$d) %*% t(test$u )
view_matrix(SigmaU_hat[1:100, 1:100], legend_title = "GradFPS\nProjection\nMatrix")
view_evec(
test$u[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
eu[1:100,],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
eu[1:100,1:10],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
test$u[1:100,1:10],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
view_evec(
test$u[1:100,1:2],
xlab = "Index of Variables", ylab = "Index of PCs", legend_title = "Factor\nLoading",
asp = 0.2, bar_height = 6, font_size = 20, pal = NULL
)
z = rmvnorm(mu, sigma = 0.1)
mu
dim(mu)
z = rmvnorm(mu, sigma = 0.1 * diag(p))
z = rmvnorm(mu, sigma = 0.1 * diag(rep(1,p)))
z = matrix(0, n, p)
##### Synthetic experiments
library(VGAM)
library("nnTensor")
setwd("~/Documents/tensor-topic-modeling/")
source("data_generation.R")
set.seed(1234)
library(einsum)
K_p = 4 #### nb of topics
p = 50   #### nb of words
t = 10 ### nb of time points
K_t = 2  #### nb of time "topics"
n = 30 ### nb of samples
test_1 <- synthetic_dataset_creation(n, t,
p, K_n, K_t, K_p, alpha_dirichlet = 1,
n_max_zipf = 5 * 1e3,
a_zipf = 1,
offset_zipf = 2.7,
n_anchors = 2,
delta_anchor = 1,
N = 500,
seed = 123, vary_by_topic=FALSE,
sparsity = FALSE)
##### Synthetic experiments
library(VGAM)
library("nnTensor")
setwd("~/Documents/tensor-topic-modeling/")
source("data_generation.R")
set.seed(1234)
library(einsum)
K_p = 4 #### nb of topics
p = 50   #### nb of words
t = 10 ### nb of time points
K_t = 2  #### nb of time "topics"
n = 30 ### nb of samples
K_n = 3  #### nb of reviewer "personae"
synthetic_dataset_creation2 <- function(n, t, p,
K_n, K_t, K_p, alpha_dirichlet = 1,
n_anchors = 0,
delta_anchor = 1,
nb_words_per_doc = 500,
seed = 123){
##############
set.seed(seed)
core  = rdiric(n = K_n * K_t, shape = rep(1, K_p), dimension=K_p)
core = array(core, dim = c(K_n, K_t, K_p))
A1 = rdiric(n=n, shape = rep(1, K_n), dimension=K_n)
A2 = rdiric(n=t, shape = rep(1, K_t), dimension=K_t)
A3 = t(rdiric(n=K_p, shape = rep(1, p), dimension=p))
# apply(A1,1,sum)
# apply(A2,1,sum)
# apply(A3,2,sum)
A1 <- anchor_document(A1, n_anchors=n_anchors, delta_anchor=delta_anchor)  ### makes sure that there is an anchor in the set
A1 <- diag(1/apply(A1,1,sum)) %*% A1
A2 <- anchor_document(A2, n_anchors=n_anchors, delta_anchor=delta_anchor) ### makes sure that there is an anchor in the set
A2 <- diag(1/apply(A2,1,sum)) %*% A2
A3 <- anchor_document(A3, n_anchors=n_anchors, delta_anchor=delta_anchor)
A3 <- A3 %*% diag(1/apply(A3,2,sum))
G <- rdiric(K_n * K_t, rep(alpha_dirichlet, K_p))
G <- array(G, dim = c(K_n, K_t, K_p))
data = einsum("ni, ijk -> njk", A1, G)
data = einsum("Tj, njk -> nTk", A2, data)
data = einsum("Rk, nTk -> nTR", A3, data)
D0 = as.tensor(data)
Y =array(0, dim= c(n, t, p))
for (i in 1:n){
for (j in 1:t){
Y[i,j,] = rmultinom(size=nb_words_per_doc, n=1, prob = D0@data[i,j,])
}
}
Y = as.tensor(Y)
### check whether some words are unused
Y3 = matrization_tensor(Y, 3)
selected_words <- which(apply(Y3,1, sum) >0 )
Y <- Y[,,selected_words]
A3  <- A3[selected_words, ] %*% diag(1/ apply(A3[selected_words, ],2, sum))
return(list(D=Y, A1=A1, A2=A2, A3=A3,
D0=D0, G=G))
}
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
test_1 <- synthetic_dataset_creation(n, t,
p, K_n, K_t, K_p, alpha_dirichlet = 1,
n_max_zipf = 5 * 1e3,
a_zipf = 1,
offset_zipf = 2.7,
n_anchors = 2,
delta_anchor = 1,
N = 500,
seed = 123, vary_by_topic=FALSE,
sparsity = FALSE)
test_1 <- synthetic_dataset_creation(n, t,
p, K_n, K_t, K_p, alpha_dirichlet = 1,
n_max_zipf = 5 * 1e3,
a_zipf = 1,
offset_zipf = 2.7,
n_anchors = 2,
delta_anchor = 1,
N = 500,
seed = 123, vary_by_topic=FALSE,
sparsity = TRUE)
D3_0 <- k_unfold(test$D0,3)
dim(D3_0)
sum(test$D0@data[1,1,])
apply(D3_0@data,2,sum)
##### Synthetic experiments
library(VGAM)
library("nnTensor")
setwd("~/Documents/tensor-topic-modeling/")
source("data_generation.R")
source("compute_error_stats.r")
source("utils_alternative_methods")
source("utils_alternative_methods.r")
set.seed(1234)
library(einsum)
K_p = 4 #### nb of topics
p = 50   #### nb of words
t = 10 ### nb of time points
K_t = 2  #### nb of time "topics"
n = 30 ### nb of samples
K_n = 3  #### nb of reviewer "personae"
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
##### Synthetic experiments
library(VGAM)
library("nnTensor")
setwd("~/Documents/tensor-topic-modeling/")
source("data_generation.R")
source("compute_error_stats.r")
source("utils_alternative_methods.r")
set.seed(1234)
library(einsum)
K_p = 4 #### nb of topics
p = 50   #### nb of words
t = 10 ### nb of time points
K_t = 2  #### nb of time "topics"
n = 30 ### nb of samples
K_n = 3  #### nb of reviewer "personae"
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
A3 <- anchor_document(A3, n_anchors=n_anchors, delta_anchor=delta_anchor)
A3 <- A3 %*% diag(1/apply(A3,2,sum))
G <- rdiric(K_n * K_t, rep(alpha_dirichlet, K_p))
G <- array(G, dim = c(K_n, K_t, K_p))
data = einsum("ni, ijk -> njk", A1, G)
data = einsum("Tj, njk -> nTk", A2, data)
data = einsum("Rk, nTk -> nTR", A3, data)
D0 = as.tensor(data)
Y =array(0, dim= c(n, t, p))
for (i in 1:n){
for (j in 1:t){
Y[i,j,] = rmultinom(size=nb_words_per_doc, n=1, prob = D0@data[i,j,])
}
}
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
post_process_core_ntd
post_process_core_ntd
Y =array(0, dim= c(n, t, p))
nb_words_per_doc
i
i=1
j=1
D0@data[i,j,]
D0 = as.tensor(data)
D0@data[i,j,]
rmultinom(size=nb_words_per_doc, n=1, prob = D0@data[i,j,]
)
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
which(apply(Y3, 1, sum) >0 )
selected_words <- which(apply(Y3@data, 1, sum) >0 )
selected_words
Y <- Y[, , selected_words]
A3  <- A3[selected_words, ] %*% diag(1/ apply(A3[selected_words, ], 2, sum))
source("data_generation.R")
source("compute_error_stats.r")
source("utils_alternative_methods.r")
set.seed(1234)
library(einsum)
K_p = 4 #### nb of topics
p = 50   #### nb of words
t = 10 ### nb of time points
K_t = 2  #### nb of time "topics"
n = 30 ### nb of samples
K_n = 3  #### nb of reviewer "personae"
test <- synthetic_dataset_creation2(n, t, p,
K_n, K_t, K_p,
alpha_dirichlet = 1,
n_anchors = 2,
delta_anchor = 1,
nb_words_per_doc = 500, seed = 1234)
D3_0 <- k_unfold(test$D0,3)
dim(D3_0)
sum(test$D0@data[1,1,])
apply(D3_0@data,2,sum)
test_NTD <- NTD(test$D,rank=c(K_n,K_t,K_p),algorithm="KL",init="NMF",num.iter=2)
hat_A_ntd = test_NTD$A
A1_hat_ntd <- post_process_ntd(hat_A_ntd$A1)
source("utils_alternative_methods.r")
A1_hat_ntd <- post_process_ntd(hat_A_ntd$A1)
A1_hat_ntd <- post_process_latent_ntd(hat_A_ntd$A1)
hat_A_ntd$A1
post_process_latent_ntd(hat_A_ntd$A1)
post_process_latent_ntd(hat_A_ntd$A1)
source("~/Documents/tensor-topic-modeling/utils_alternative_methods.r", echo=TRUE)
A1_hat_ntd <- post_process_latent_ntd(hat_A_ntd$A1)
A2_hat_ntd <- post_process_latent_ntd(hat_A_ntd$A2)
A3_hat_ntd <- t(post_process_latent_ntd(t(hat_A_ntd$A3)))
hat_core@data =post_process_core_ntd(hat_core)
hat_core@data =post_process_core_ntd(test_NTD$S)
test_NTD$S
typeof(post_process_core_ntd(test_NTD$S))
source("~/Documents/tensor-topic-modeling/utils_alternative_methods.r", echo=TRUE)
hat_core = post_process_core_ntd(test_NTD$S)
t = test_NTD$S
t@data
source("~/Documents/tensor-topic-modeling/utils_alternative_methods.r", echo=TRUE)
hat_core = post_process_core_ntd(test_NTD$S)
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
nb_words_per_doc = 500
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
hatA1 = A1_hat_ntd
A1 = test$A1
error_res1 = matrix_lp_distance(hatA1, A1, lp=1)
errorl1_1 = error_res1$error
debugSource("~/Documents/tensor-topic-modeling/compute_error_stats.r", echo=TRUE)
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
hatA1
A1
matrix_lp_distance(hatA1, A1, lp=1)
errorl1_1 = error_res1$error
debugSource("~/Documents/tensor-topic-modeling/compute_error_stats.r", echo=TRUE)
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
errorl1_1
error_temp <- error_update(error=errorl1_1, K=K_n,
n=n,
p=p, t=t, nb_words_per_doc = nb_words_per_doc,
mode="A1",method=method, time=time)
mode
mode = "A1"
method
time
debugSource("~/Documents/tensor-topic-modeling/compute_error_stats.r", echo=TRUE)
error_temp <- error_update(error=errorl1_1, K=K_n,
n=n,
p=p, t=t, nb_words_per_doc = nb_words_per_doc,
mode="A1",method=method, time=time)
K
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error
K
data.frame("error" = error,
"K" = K,
"n" = n,
"p" = p,
"t" = t,
"nb_words_per_doc" = nb_words_per_doc,
"mode" = mode,
"method" = method,
"time" = time)
data.frame
data.frame("x"=1)
method
nb_words_per_doc
t
t = 10
error_temp <- error_update(error=errorl1_1, K=K_n,
n=n,
p=p, t=t, nb_words_per_doc = nb_words_per_doc,
mode="A1",method=method, time=time)
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core@data, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error <- compute_error_stats(A1_hat_ntd,
test$A1, A2_hat_ntd,
test$A2, A3_hat_ntd, test$A3,
hat_core, test$G, K_n, K_t,
K_p,  n, t, p,
nb_words_per_doc, 0, method="NTD", c())
error
#### This is probably not an optimal way of doing things
lda1 <- LDA(t(D3@data), k = K_t, control = list(seed = 1234), method = 'VEM')
D3
#### This is probably not an optimal way of doing things'
D3 <- k_unfold(test$D, 3)
lda1 <- LDA(t(D3@data), k = K_t, control = list(seed = 1234), method = 'VEM')
